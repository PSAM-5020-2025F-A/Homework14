{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework14\n",
    "\n",
    "Pre-Trained Transformer Models and Embeddings\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Practice setting up classification and clustering modeling task from scratch\n",
    "- Experiment with pre-trained transformer models for embedding and feature extraction\n",
    "- Get more familiar with the `argmax()` and `pairwise_distance()` functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qO- https://github.com/PSAM-5020-2025F-A/5020-utils/releases/latest/download/flowers102.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from numpy import argsort\n",
    "from os import listdir\n",
    "from PIL import Image as PImage\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from torch import nn, Tensor, no_grad\n",
    "from torch import float32 as t_float32, uint8 as t_uint8\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Shot Classification\n",
    "\n",
    "## Intro\n",
    "\n",
    "We're going to leverage the general knowledge and patterns learned by pre-trained deep learning models to create a one-shot classifier model.\n",
    "\n",
    "One-shot classifiers are models that learn how to describe/detect objects by just looking at one example from each class.\n",
    "\n",
    "The overall flow for doing this using image embeddings can be something like:\n",
    "- extract embeddings for all images in our dataset\n",
    "- training: for each class, average a small number of embeddings to create class embeddings\n",
    "- these embeddings now represent information about the classes we're trying to identify\n",
    "- predicting: find the closest class embedding to each image embedding in the dataset\n",
    "\n",
    "This is similar to one of the examples in our [Week 14](https://github.com/PSAM-5020-2025F-A/WK14) notebook where we used words to classify images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "We're going to classify the [Oxford Flowers Dataset](https://www.robots.ox.ac.uk/~vgg/data/flowers/). This is a dataset made up of images of flowers and their names.\n",
    "\n",
    "All of the images we're going to be working with should have been downloaded by the first cell, into separate `train` and `test` subdirectories inside `./data/image/flowers102/`.\n",
    "\n",
    "The images all have the same (or similar) height, but very different widths. Depending on the architecture/model that we choose to use for  embedding, this might be something we have to standardize. Transformer architectures and their preprocessors will automatically deal with these differences, where CNN models are a bit more strict.\n",
    "\n",
    "Let's start by defining a function that helps parse the classification label from file names or paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filepath_to_label(filepath):\n",
    "  return filepath.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "Now we have to define an embedding model, and (probably) a pre-processing strategy.\n",
    "\n",
    "What wee need here is a pre-trained model that is able to turn images of various sizes into feature lists of fixed-length.\n",
    "\n",
    "Our [Week 14](https://github.com/PSAM-5020-2025F-A/WK14) notebook has a couple of examples of how to use `ResNet`, `CLIP` and `SigLIP` models to do this, but there are other options that could be used. Since we're not doing any text processing, any kind of deep learning visual model can (theoretically) be used.\n",
    "\n",
    "Some other examples: [Nomic Vision](https://huggingface.co/nomic-ai/nomic-embed-vision-v1.5), [EfficientNet](https://pytorch.org/hub/nvidia_deeplearningexamples_efficientnet/), [ViT](https://huggingface.co/google/vit-base-patch16-224-in21k), [DINOv3](https://huggingface.co/facebook/dinov3-vitl16-pretrain-lvd1689m), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: define model and pre-processing routine/function/strategy for images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data\n",
    "\n",
    "Now we process the train data. Fun.\n",
    "\n",
    "There are many ways to do this, but one possible strategy is to go through all of the files inside the `./data/image/flowers102/train` directory and append each image's label and embedding to separate lists, called `train_labels` and `train_embeddings`.\n",
    "\n",
    "Then, create a `DataFrame` using the embeddings and add the labels to the same `DataFrame`.\n",
    "\n",
    "Depending on the model chosen, this can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: extract labels and embeddings for each image in the training dataset\n",
    "\n",
    "TRAIN_DIR = \"./data/image/flowers102/train\"\n",
    "\n",
    "train_fnames = sorted([f for f in listdir(TRAIN_DIR) if f.endswith(\"jpg\")])\n",
    "\n",
    "train_labels = []\n",
    "train_embeddings = []\n",
    "\n",
    "for fname in train_fnames:\n",
    "  # TODO: replace this with code\n",
    "  pass\n",
    "\n",
    "# TODO: combine the lists into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Questions\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "How many images are in the training dataset ?<br>\n",
    "How many <em>\"features\"</em> ?<br>\n",
    "How many unique classes do we have for this data ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">ADD ANSWER TO THIS CELL</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "Repeat the above process for the files inside the `./data/image/flowers102/test` directory: append each image's label and embedding to separate lists, called `test_labels` and `test_embeddings`.\n",
    "\n",
    "Since we won't do any other kind of processing on this data, it's not as important to combine the labels and embeddings into a `DataFrame`.\n",
    "\n",
    "And again, this can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Repeat label and embedding extraction for all images in the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Questions\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "How many images are in the test dataset ?<br>\n",
    "Anything odd or unusual about this ?<br>\n",
    "Is the test dataset balanced ?<br>\n",
    "Does it matter ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">ADD ANSWER TO THIS CELL</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "This is the unconventional part. We don't need to train any models, but use the already-trained one to derive some information about our training data that can then be used to make new predictions.\n",
    "\n",
    "There are different ways to do this, but a recommended strategy here could be:\n",
    "- get a list of unique labels in our dataset\n",
    "- iterate over the labels, filter the `DataFrame` by label and compute an average embedding for all images of each label\n",
    "- these are now class embeddings, as they hold aggregate information about multiple instances of each class\n",
    "- we should end up with as many class embeddings as there are unique labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: create class embeddings by averaging embeddings for all images of each label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Evaluate\n",
    "\n",
    "We have class embeddings, and we have instance embeddings from all images in the test dataset.\n",
    "\n",
    "We can use the `euclidean_distances()` function to calculate pairwise distances between each image and each class embedding.\n",
    "\n",
    "Then, we'll go through each image and get the index of the class embedding that is closest to its image embedding.\n",
    "\n",
    "We can create a `predictions` list to compare to the `test_labels` list we extracted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: use euclidean_distances() and argsort() to determine the closest class for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "So... What happened ?<br>\n",
    "What are some advantages and disadvantages of using this strategy for classification ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">ADD ANSWER TO THIS CELL</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
